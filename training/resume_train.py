# -*- coding: utf-8 -*-
"""resume_train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XHS9dhBztdUBsqJySDmilmOUVVgAT0eR
"""

"""
resume_train.py â€” Train a TF-IDF + Keras model for resume classification.
"""

import os, glob, unicodedata, pickle, json
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks, optimizers, losses, metrics

try:
    from PyPDF2 import PdfReader
except ImportError:
    from pypdf import PdfReader

# ---------- Helpers ----------
def read_text(path: str) -> str:
    if path.lower().endswith(".txt"):
        return open(path, "r", encoding="utf-8", errors="ignore").read()
    if path.lower().endswith(".pdf"):
        text = []
        try:
            reader = PdfReader(path)
            for p in reader.pages:
                text.append(p.extract_text() or "")
        except Exception:
            return ""
        return "\n".join(text)
    return ""

def clean_text(txt: str) -> str:
    txt = unicodedata.normalize("NFKC", txt or "")
    return " ".join(txt.split())

def load_dataset(data_root: str):
    texts, labels = [], []
    classes = sorted([d for d in os.listdir(data_root) if os.path.isdir(os.path.join(data_root,d))])
    for c in classes:
        for fp in glob.glob(os.path.join(data_root, c, "*")):
            t = clean_text(read_text(fp))
            if t:
                texts.append(t)
                labels.append(c)
    return texts, labels, classes

# ---------- Model ----------
def build_model(input_dim: int, n_classes: int):
    m = models.Sequential()
    m.add(layers.Input(shape=(input_dim,), dtype="float32"))
    m.add(layers.Dense(512, activation="relu"))
    m.add(layers.Dropout(0.3))
    m.add(layers.Dense(256, activation="relu"))
    m.add(layers.Dropout(0.3))
    m.add(layers.Dense(n_classes, activation="softmax"))
    m.compile(
        optimizer=optimizers.Adam(2e-3),
        loss=losses.SparseCategoricalCrossentropy(),
        metrics=[metrics.SparseCategoricalAccuracy(name="acc")]
    )
    return m

# ---------- Training ----------
def train(data_root: str, out_dir: str, epochs=10):
    os.makedirs(out_dir, exist_ok=True)
    texts, labels, class_names = load_dataset(data_root)
    print(f"Loaded {len(texts)} docs across {len(class_names)} classes")

    le = LabelEncoder()
    y = le.fit_transform(labels)

    X_train_txt, X_val_txt, y_train, y_val = train_test_split(
        texts, y, test_size=0.15, random_state=42, stratify=y
    )

    vec = TfidfVectorizer(max_features=60000, ngram_range=(1,2), min_df=2)
    X_train = vec.fit_transform(X_train_txt)
    X_val   = vec.transform(X_val_txt)

    model = build_model(X_train.shape[1], len(class_names))

    class_weights = compute_class_weight("balanced", classes=np.unique(y_train), y=y_train)
    class_weights = {i: float(w) for i,w in enumerate(class_weights)}

    ckpt = os.path.join(out_dir, "resume_classifier.h5")
    cbs = [
        callbacks.EarlyStopping(monitor="val_acc", patience=3, restore_best_weights=True),
        callbacks.ModelCheckpoint(ckpt, monitor="val_acc", save_best_only=True)
    ]

    history = model.fit(
        X_train.toarray(), y_train,
        validation_data=(X_val.toarray(), y_val),
        epochs=epochs,
        batch_size=32,
        class_weight=class_weights,
        callbacks=cbs,
        verbose=2
    )

    # Evaluate
    val_probs = model.predict(X_val.toarray(), verbose=0)
    val_pred = val_probs.argmax(axis=1)
    print("Val accuracy:", (val_pred == y_val).mean())

    # Save artifacts
    model.save(os.path.join(out_dir, "resume_classifier_final.h5"))
    with open(os.path.join(out_dir, "tfidf_vectorizer.pkl"), "wb") as f: pickle.dump(vec, f)
    with open(os.path.join(out_dir, "label_encoder.pkl"), "wb") as f: pickle.dump(le, f)
    pd.Series(class_names).to_csv(os.path.join(out_dir, "class_names.csv"), index=False)

    with open(os.path.join(out_dir, "metrics.json"), "w") as f:
        json.dump({"history": history.history}, f)

if __name__ == "__main__":
    DATA_ROOT = "./resumes_dataset"   # ðŸ‘ˆ put your dataset root here
    OUT_DIR   = "./models_tf"
    train(DATA_ROOT, OUT_DIR, epochs=10)

















