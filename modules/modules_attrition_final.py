# -*- coding: utf-8 -*-
"""modules_attrition_final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wRT7cenVAglFGSSTOd2uPkvXVj1SVIx5
"""

# modules/modules_attrition_final.py
from __future__ import annotations
from pathlib import Path
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline as SkPipeline
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.metrics import (
    roc_auc_score, average_precision_score, f1_score, balanced_accuracy_score,
    confusion_matrix
)
from sklearn.inspection import permutation_importance
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE


# --------- Data loader ----------
def load_dataset(path: str | Path | None = None) -> pd.DataFrame:
    if path is None:
        path = Path(__file__).resolve().parents[1] / "data" / "WA_Fn-UseC_-HR-Employee-Attrition.csv"
    return pd.read_csv(path, sep=None, engine="python")


# --------- Core prep + features ----------
def _prepare(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # Drop IDs / constants that add noise
    drop_cols = ['EmployeeNumber','Over18','StandardHours','EmployeeCount','DailyRate','HourlyRate','MonthlyRate']
    df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")

    # Target
    assert "Attrition" in df.columns, "Column 'Attrition' not found"
    df["Attrition_numerical"] = df["Attrition"].map({"Yes":1,"No":0}).astype(int)

    # Light feature engineering
    if "OverTime" in df.columns:
        df["OverTime_flag"] = df["OverTime"].map({"Yes":1, "No":0})
    if {"DistanceFromHome","OverTime_flag"}.issubset(df.columns):
        df["CommuteStress"] = df["DistanceFromHome"] * df["OverTime_flag"]
    if {"MaritalStatus","Age"}.issubset(df.columns):
        df["IsSingleAndYounger"] = ((df["MaritalStatus"]=="Single") & (df["Age"]<35)).astype(int)
    if {"Age","MonthlyIncome"}.issubset(df.columns):
        df["Age_Income"] = df["Age"] * df["MonthlyIncome"]
    if {"JobLevel","JobSatisfaction"}.issubset(df.columns):
        df["JobLevel_Satisfaction"] = df["JobLevel"] * df["JobSatisfaction"]
    if {"YearsAtCompany","YearsSinceLastPromotion"}.issubset(df.columns):
        df["YearsAtCompany_Promotion"] = df["YearsAtCompany"] * df["YearsSinceLastPromotion"]

    return df


def _build_preprocessor(X: pd.DataFrame):
    cat_cols = X.select_dtypes(include=["object","category"]).columns.tolist()
    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()

    preprocessor = ColumnTransformer([
        ("num", SkPipeline([("impute", SimpleImputer(strategy="median")), ("scale", StandardScaler())]), num_cols),
        ("cat", SkPipeline([("impute", SimpleImputer(strategy="most_frequent")),
                            ("ohe", OneHotEncoder(handle_unknown="ignore"))]), cat_cols),
    ])
    return preprocessor, num_cols, cat_cols


# --------- Existing API (kept) ----------
def train_select_and_test(df: pd.DataFrame | None = None, k_features: int | None = None, random_state: int = 42):
    if df is None:
        df = load_dataset().copy()
    else:
        df = df.copy()

    df = _prepare(df)
    TARGET = "Attrition_numerical"
    y = df[TARGET]
    X = df.drop(columns=[TARGET, "Attrition"], errors="ignore")

    preprocessor, num_cols, cat_cols = _build_preprocessor(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, stratify=y, random_state=random_state
    )

    # Optional outlier filtering on numeric train
    if len(num_cols):
        iso = IsolationForest(random_state=random_state, contamination=0.03)
        iso.fit(X_train[num_cols])
        keep = (iso.predict(X_train[num_cols]) == 1)
        X_train, y_train = X_train.loc[keep], y_train.loc[keep]

    # k for SelectKBest
    if k_features is None:
        _fit = preprocessor.fit(X_train)
        n_after = _fit.transform(X_train).shape[1]
        k_features = min(40, n_after) if n_after > 0 else "all"

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)
    scoring = {"roc_auc":"roc_auc", "pr_auc":"average_precision", "f1":"f1", "bal_acc":"balanced_accuracy"}

    log_reg = ImbPipeline([
        ("prep", preprocessor),
        ("smote", SMOTE(random_state=random_state, k_neighbors=5)),
        ("sel", SelectKBest(score_func=mutual_info_classif, k=k_features)),
        ("clf", LogisticRegression(max_iter=2000, random_state=random_state)),
    ])

    rf = ImbPipeline([
        ("prep", preprocessor),
        ("smote", SMOTE(random_state=random_state, k_neighbors=5)),
        ("sel", SelectKBest(score_func=mutual_info_classif, k=k_features)),
        ("clf", RandomForestClassifier(
            n_estimators=400, min_samples_split=5, min_samples_leaf=2,
            random_state=random_state, n_jobs=-1
        )),
    ])

    cv_log = cross_validate(log_reg, X_train, y_train, cv=skf, scoring=scoring, n_jobs=-1)
    cv_rf  = cross_validate(rf,      X_train, y_train, cv=skf, scoring=scoring, n_jobs=-1)

    mm = lambda d: {k.replace("test_",""): float(np.mean(v)) for k,v in d.items() if k.startswith("test_")}
    cv_report = {"log_reg": mm(cv_log), "random_forest": mm(cv_rf)}

    best_is_log = cv_report["log_reg"]["roc_auc"] >= cv_report["random_forest"]["roc_auc"]
    best_name   = "LogisticRegression" if best_is_log else "RandomForest"
    best_model  = log_reg if best_is_log else rf

    best_model.fit(X_train, y_train)
    proba = best_model.predict_proba(X_test)[:, 1]
    pred  = (proba >= 0.5).astype(int)

    test_report = {
        "roc_auc": float(roc_auc_score(y_test, proba)),
        "pr_auc":  float(average_precision_score(y_test, proba)),
        "f1":      float(f1_score(y_test, pred)),
        "bal_acc": float(balanced_accuracy_score(y_test, pred)),
        "confusion_matrix": confusion_matrix(y_test, pred, labels=[0,1]).tolist(),
    }
    return cv_report, best_name, test_report


# --------- New: fit + explain (returns artifacts for dashboard) ----------
def fit_best_and_explain(df: pd.DataFrame | None = None, random_state: int = 42,
                         k_features: int | None = None,
                         n_repeats: int = 12):
    """
    Returns:
      fitted_model, cv_report, best_name, test_report, importance_df, cm_df, X_test, y_test, proba, pred, test_index
    """
    if df is None:
        df = load_dataset().copy()
    else:
        df = df.copy()

    df = _prepare(df)
    TARGET = "Attrition_numerical"
    y = df[TARGET]
    X = df.drop(columns=[TARGET, "Attrition"], errors="ignore")

    preprocessor, num_cols, cat_cols = _build_preprocessor(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, stratify=y, random_state=random_state
    )

    if len(num_cols):
        iso = IsolationForest(random_state=random_state, contamination=0.03)
        iso.fit(X_train[num_cols])
        keep = (iso.predict(X_train[num_cols]) == 1)
        X_train, y_train = X_train.loc[keep], y_train.loc[keep]

    if k_features is None:
        _fit = preprocessor.fit(X_train)
        n_after = _fit.transform(X_train).shape[1]
        k_features = min(40, n_after) if n_after > 0 else "all"

    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)
    scoring = {"roc_auc":"roc_auc", "pr_auc":"average_precision", "f1":"f1", "bal_acc":"balanced_accuracy"}

    log_reg = ImbPipeline([
        ("prep", preprocessor),
        ("smote", SMOTE(random_state=random_state, k_neighbors=5)),
        ("sel", SelectKBest(score_func=mutual_info_classif, k=k_features)),
        ("clf", LogisticRegression(max_iter=2000, random_state=random_state)),
    ])

    rf = ImbPipeline([
        ("prep", preprocessor),
        ("smote", SMOTE(random_state=random_state, k_neighbors=5)),
        ("sel", SelectKBest(score_func=mutual_info_classif, k=k_features)),
        ("clf", RandomForestClassifier(
            n_estimators=400, min_samples_split=5, min_samples_leaf=2,
            random_state=random_state, n_jobs=-1
        )),
    ])

    cv_log = cross_validate(log_reg, X_train, y_train, cv=skf, scoring=scoring, n_jobs=-1)
    cv_rf  = cross_validate(rf,      X_train, y_train, cv=skf, scoring=scoring, n_jobs=-1)

    mm = lambda d: {k.replace("test_",""): float(np.mean(v)) for k,v in d.items() if k.startswith("test_")}
    cv_report = {"log_reg": mm(cv_log), "random_forest": mm(cv_rf)}

    best_is_log = cv_report["log_reg"]["roc_auc"] >= cv_report["random_forest"]["roc_auc"]
    best_name   = "LogisticRegression" if best_is_log else "RandomForest"
    model       = log_reg if best_is_log else rf

    model.fit(X_train, y_train)
    proba = model.predict_proba(X_test)[:, 1]
    pred  = (proba >= 0.5).astype(int)

    test_report = {
        "roc_auc": float(roc_auc_score(y_test, proba)),
        "pr_auc":  float(average_precision_score(y_test, proba)),
        "f1":      float(f1_score(y_test, pred)),
        "bal_acc": float(balanced_accuracy_score(y_test, pred)),
    }
    cm = confusion_matrix(y_test, pred, labels=[0,1])
    cm_df = pd.DataFrame(cm, index=["true_No","true_Yes"], columns=["pred_No","pred_Yes"])

    # Permutation importance at original column level
    pi = permutation_importance(model, X_test, y_test, scoring="roc_auc",
                                n_repeats=n_repeats, random_state=random_state)
    imp_df = (pd.DataFrame({"feature": X_test.columns, "importance": pi.importances_mean})
                .sort_values("importance", ascending=False)
                .reset_index(drop=True))

    return model, cv_report, best_name, test_report, imp_df, cm_df, X_test, y_test, proba, pred, X_test.index